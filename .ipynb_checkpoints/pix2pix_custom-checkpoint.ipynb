{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom pix2pix test\n",
    "\n",
    "## Main fork: https://github.com/tjwei/GANotebooks\n",
    "\n",
    "Custom implementation of the pix2pix algorithm for GPR B-scan to ground truth image transform.\n",
    "Main source code base from https://github.com/tjwei/GANotebooks and \n",
    "https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/pix2pix\n",
    "rewritten for a more appropriate and extended implementation.\n",
    "\n",
    "## System requirements:\n",
    "Keras - with Tensorflow backend\n",
    "\n",
    "Pilow - image loading\n",
    "\n",
    "pydot - display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow' # use tensorflow. Original code can swap between theano, tensorflow and cntk\n",
    "#os.environ['THEANO_FLAGS']='floatX=float32,device=cuda,optimizer=fast_run,dnn.library_path=/usr/lib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "channel_axis=-1\n",
    "channel_first = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Keras Layers and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_init = RandomNormal(0, 0.02)\n",
    "gamma_init = RandomNormal(1., 0.02) # for batch normalization\n",
    "\n",
    "def conv2d(f, *a, **k):\n",
    "    return Conv2D(f, kernel_initializer = conv_init, *a, **k)\n",
    "def batchnorm():\n",
    "    return BatchNormalization(momentum=0.9, axis=channel_axis, epsilon=1.01e-5,\n",
    "    gamma_initializer = gamma_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# U-net Generator\n",
    "\n",
    "def unet(im_size, nc_in=1, nc_out=1, ngf=64, fixed_input_size=True):\n",
    "    \n",
    "    # Encoder\n",
    "    # C64-C128-C256-C512-C512-C512-C512-C512. Hell of a bottleneck for a 256 image\n",
    "    \n",
    "    # convolution\n",
    "    # filter size 4, stride 2. Downsamples by 2 per level. 8 levels\n",
    "    \n",
    "    # batch norm\n",
    "    # no batch norm in the first C64 layer\n",
    "    \n",
    "    # relu\n",
    "    # leaky alpha 0.2\n",
    "    \n",
    "    # Decoder\n",
    "    # CD512-CD512-CD512-C512-C512-C256-C128-C64\n",
    "\n",
    "    # convolution\n",
    "    \n",
    "    # batch norm\n",
    "    \n",
    "    # drop out\n",
    "    # 0.5\n",
    "    \n",
    "    # relu\n",
    "\n",
    "    \n",
    "    max_nf = 8*ngf    \n",
    "    \n",
    "    def block(x, s, nf_in, use_batchnorm=True, nf_out=None, nf_next=None):\n",
    "        # print(\"block\",x,s,nf_in, use_batchnorm, nf_out, nf_next)\n",
    "        assert s>=2 and s%2==0\n",
    "        if nf_next is None:\n",
    "            nf_next = min(nf_in*2, max_nf)\n",
    "        if nf_out is None:\n",
    "            nf_out = nf_in\n",
    "        x = conv2d(nf_next, kernel_size=4, strides=2, use_bias=(not (use_batchnorm and s>2)),\n",
    "                   padding=\"same\", name = 'conv_{0}'.format(s)) (x)\n",
    "        if s>2:\n",
    "            if use_batchnorm:\n",
    "                x = batchnorm()(x, training=1)\n",
    "            x2 = LeakyReLU(alpha=0.2)(x)\n",
    "            x2 = block(x2, s//2, nf_next)\n",
    "            x = Concatenate(axis=channel_axis)([x, x2])            \n",
    "        x = LeakyReLU(alpha=0)(x)\n",
    "        x = Conv2DTranspose(nf_out, kernel_size=4, strides=2, use_bias=not use_batchnorm,\n",
    "                            kernel_initializer = conv_init,          \n",
    "                            name = 'convt.{0}'.format(s))(x)        \n",
    "        x = Cropping2D(1)(x)\n",
    "        if use_batchnorm:\n",
    "            x = batchnorm()(x, training=1)\n",
    "        if s <=8:\n",
    "            x = Dropout(0.5)(x, training=1)\n",
    "        return x\n",
    "    \n",
    "    s = im_size if fixed_input_size else None\n",
    "    _ = inputs = Input(shape=(s, s, nc_in))        \n",
    "    _ = block(_, im_size, nc_in, False, nf_out=nc_out, nf_next=ngf)\n",
    "    _ = Activation('tanh')(_)\n",
    "    return Model(inputs=inputs, outputs=[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def BASIC_D(nc_in, nc_out, ndf, max_layers=3):\n",
    "    \"\"\"DCGAN_D(nc, ndf, max_layers=3)\n",
    "       nc: channels\n",
    "       ndf: filters of the first layer\n",
    "       max_layers: max hidden layers\n",
    "    \"\"\"    \n",
    "    if channel_first:\n",
    "        input_a, input_b =  Input(shape=(nc_in, None, None)), Input(shape=(nc_out, None, None))\n",
    "    else:\n",
    "        input_a, input_b = Input(shape=(None, None, nc_in)), Input(shape=(None, None, nc_out))\n",
    "    _ = Concatenate(axis=channel_axis)([input_a, input_b])\n",
    "    _ = conv2d(ndf, kernel_size=4, strides=2, padding=\"same\", name = 'First') (_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    for layer in range(1, max_layers):        \n",
    "        out_feat = ndf * min(2**layer, 8)\n",
    "        _ = conv2d(out_feat, kernel_size=4, strides=2, padding=\"same\", \n",
    "                   use_bias=False, name = 'pyramid.{0}'.format(layer)             \n",
    "                        ) (_)\n",
    "        _ = batchnorm()(_, training=1)        \n",
    "        _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    out_feat = ndf*min(2**max_layers, 8)\n",
    "    _ = ZeroPadding2D(1)(_)\n",
    "    _ = conv2d(out_feat, kernel_size=4,  use_bias=False, name = 'pyramid_last') (_)\n",
    "    _ = batchnorm()(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    # final layer\n",
    "    _ = ZeroPadding2D(1)(_)\n",
    "    _ = conv2d(1, kernel_size=4, name = 'final'.format(out_feat, 1), \n",
    "               activation = \"sigmoid\") (_)    \n",
    "    return Model(inputs=[input_a, input_b], outputs=_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_256 (Conv2D)               (None, 128, 128, 64) 3136        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 128, 128, 64) 0           conv_256[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_128 (Conv2D)               (None, 64, 64, 128)  131072      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 64, 64, 128)  512         conv_128[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 64, 64, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_64 (Conv2D)                (None, 32, 32, 256)  524288      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 32, 256)  1024        conv_64[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 32, 32, 256)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_32 (Conv2D)                (None, 16, 16, 512)  2097152     leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 512)  2048        conv_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 16, 16, 512)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 8, 8, 512)    4194304     leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 512)    2048        conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 8, 8, 512)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, 4, 4, 512)    4194304     leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 4, 4, 512)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 2, 2, 512)    4194304     leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 2, 2, 512)    2048        conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 2, 2, 512)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 1, 1, 512)    4194816     leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 1, 1, 512)    0           conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "convt.2 (Conv2DTranspose)       (None, 4, 4, 512)    4194304     leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_17 (Cropping2D)      (None, 2, 2, 512)    0           convt.2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 2, 2, 512)    2048        cropping2d_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 2, 2, 512)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 2, 2, 1024)   0           batch_normalization_32[0][0]     \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 2, 2, 1024)   0           concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "convt.4 (Conv2DTranspose)       (None, 6, 6, 512)    8388608     leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_18 (Cropping2D)      (None, 4, 4, 512)    0           convt.4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 4, 4, 512)    2048        cropping2d_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 4, 4, 512)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 4, 4, 1024)   0           batch_normalization_31[0][0]     \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 4, 4, 1024)   0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "convt.8 (Conv2DTranspose)       (None, 10, 10, 512)  8388608     leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_19 (Cropping2D)      (None, 8, 8, 512)    0           convt.8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 512)    2048        cropping2d_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 8, 8, 512)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 8, 8, 1024)   0           batch_normalization_30[0][0]     \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 8, 8, 1024)   0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "convt.16 (Conv2DTranspose)      (None, 18, 18, 512)  8388608     leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_20 (Cropping2D)      (None, 16, 16, 512)  0           convt.16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 16, 16, 512)  2048        cropping2d_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 1024) 0           batch_normalization_29[0][0]     \n",
      "                                                                 batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)      (None, 16, 16, 1024) 0           concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "convt.32 (Conv2DTranspose)      (None, 34, 34, 256)  4194304     leaky_re_lu_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_21 (Cropping2D)      (None, 32, 32, 256)  0           convt.32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 32, 32, 256)  1024        cropping2d_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 32, 32, 512)  0           batch_normalization_28[0][0]     \n",
      "                                                                 batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)      (None, 32, 32, 512)  0           concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "convt.64 (Conv2DTranspose)      (None, 66, 66, 128)  1048576     leaky_re_lu_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_22 (Cropping2D)      (None, 64, 64, 128)  0           convt.64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 64, 64, 128)  512         cropping2d_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 64, 64, 256)  0           batch_normalization_27[0][0]     \n",
      "                                                                 batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)      (None, 64, 64, 256)  0           concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "convt.128 (Conv2DTranspose)     (None, 130, 130, 64) 262144      leaky_re_lu_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_23 (Cropping2D)      (None, 128, 128, 64) 0           convt.128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 128, 128, 64) 256         cropping2d_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 128, 128, 128 0           conv_256[0][0]                   \n",
      "                                                                 batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)      (None, 128, 128, 128 0           concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "convt.256 (Conv2DTranspose)     (None, 258, 258, 3)  6147        leaky_re_lu_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_24 (Cropping2D)      (None, 256, 256, 3)  0           convt.256[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 256, 256, 3)  0           cropping2d_24[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 54,424,387\n",
      "Trainable params: 54,414,531\n",
      "Non-trainable params: 9,856\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nc_in = 3\n",
    "nc_out = 3\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "lambda_ = 10\n",
    "\n",
    "loadSize = 286\n",
    "imageSize = 256\n",
    "batchSize = 16\n",
    "lrG = 2e-4\n",
    "lrD = 2e-4\n",
    "\n",
    "netG = unet(imageSize, nc_in, nc_out, ngf)\n",
    "netG.summary()\n",
    "config = netG.get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD = BASIC_D(nc_in, nc_out, ndf)\n",
    "netD.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "\n",
    "real_A = netG.input\n",
    "fake_B = netG.output\n",
    "netG_generate = K.function([real_A], [fake_B])\n",
    "real_B = netD.inputs[1]\n",
    "output_D_real = netD([real_A, real_B])\n",
    "output_D_fake = netD([real_A, fake_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "\n",
    "loss_fn = lambda output, target : -K.mean(K.log(output+1e-12)*target+K.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "loss_D_real = loss_fn(output_D_real, K.ones_like(output_D_real))\n",
    "loss_D_fake = loss_fn(output_D_fake, K.zeros_like(output_D_fake))\n",
    "loss_G_fake = loss_fn(output_D_fake, K.ones_like(output_D_fake))\n",
    "\n",
    "loss_L1 = K.mean(K.abs(fake_B-real_B))\n",
    "\n",
    "loss_D = loss_D_real +loss_D_fake\n",
    "training_updates = Adam(lr=lrD, beta_1=0.5).get_updates(netD.trainable_weights,[],loss_D)\n",
    "netD_train = K.function([real_A, real_B],[loss_D/2], training_updates)\n",
    "\n",
    "loss_G = loss_G_fake   + 100 * loss_L1\n",
    "training_updates = Adam(lr=lrG, beta_1=0.5).get_updates(netG.trainable_weights,[], loss_G)\n",
    "netG_train = K.function([real_A, real_B], [loss_G_fake, loss_L1], training_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "from random import randint, shuffle\n",
    "\n",
    "def load_data(file_pattern):\n",
    "    return glob.glob(file_pattern)\n",
    "def read_image(fn, direction=0):\n",
    "    im = Image.open(fn)\n",
    "    im = im.resize( (loadSize*2, loadSize), Image.BILINEAR )\n",
    "    arr = np.array(im)/255*2-1\n",
    "    w1,w2 = (loadSize-imageSize)//2,(loadSize+imageSize)//2\n",
    "    h1,h2 = w1,w2\n",
    "    imgA = arr[h1:h2, loadSize+w1:loadSize+w2, :]\n",
    "    imgB = arr[h1:h2, w1:w2, :]\n",
    "    if randint(0,1):\n",
    "        imgA=imgA[:,::-1]\n",
    "        imgB=imgB[:,::-1]\n",
    "    if channel_first:\n",
    "        imgA = np.moveaxis(imgA, 2, 0)\n",
    "        imgB = np.moveaxis(imgB, 2, 0)\n",
    "    if direction==0:\n",
    "        return imgA, imgB\n",
    "    else:\n",
    "        return imgB,imgA\n",
    "\n",
    "data = \"facades\"\n",
    "direction = 0\n",
    "trainAB = load_data('datasets/{}/train/*.jpg'.format(data))\n",
    "valAB = load_data('datasets/{}/val/*.jpg'.format(data))\n",
    "assert len(trainAB) and len(valAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch(dataAB, batchsize, direction=0):\n",
    "    length = len(dataAB)\n",
    "    epoch = i = 0\n",
    "    tmpsize = None    \n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batchsize\n",
    "        \n",
    "        # if reached end, start new epoch and start again\n",
    "        if i+size > length:\n",
    "            shuffle(dataAB)\n",
    "            i = 0\n",
    "            epoch+=1        \n",
    "        # empty list, appned with batch images    \n",
    "        dataA = []\n",
    "        dataB = []\n",
    "        for j in range(i,i+size):\n",
    "            imgA,imgB = read_image(dataAB[j], direction)\n",
    "            dataA.append(imgA)\n",
    "            dataB.append(imgB)\n",
    "        dataA = np.float32(dataA)\n",
    "        dataB = np.float32(dataB)\n",
    "        i+=size\n",
    "        tmpsize = yield epoch, dataA, dataB     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def showX(X, rows=1):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ( (X+1)/2*255).clip(0,255).astype('uint8')\n",
    "\n",
    "    int_X = int_X.reshape(-1,imageSize,imageSize, 3)\n",
    "    int_X = int_X.reshape(rows, -1, imageSize, imageSize,3).swapaxes(1,2).reshape(rows*imageSize,-1, 3)\n",
    "    display(Image.fromarray(int_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = minibatch(trainAB, batchSize, direction=direction)\n",
    "_, trainA, trainB = next(train_batch)\n",
    "showX(trainA)\n",
    "showX(trainB)\n",
    "del train_batch, trainA, trainB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def netG_gen(A):\n",
    "    return np.concatenate([netG_generate([A[i:i+1]])[0] for i in range(A.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "t0 = time.time()\n",
    "niter = 50\n",
    "gen_iterations = 0\n",
    "errL1 = epoch = errG = 0\n",
    "errL1_sum = errG_sum = errD_sum = 0\n",
    "\n",
    "display_iters = 500\n",
    "val_batch = minibatch(valAB, 6, direction)\n",
    "train_batch = minibatch(trainAB, batchSize, direction)\n",
    "\n",
    "while epoch < niter: \n",
    "    epoch, trainA, trainB = next(train_batch)        \n",
    "    errD,  = netD_train([trainA, trainB])\n",
    "    errD_sum +=errD\n",
    "\n",
    "    errG, errL1 = netG_train([trainA, trainB])\n",
    "    errG_sum += errG\n",
    "    errL1_sum += errL1\n",
    "    gen_iterations+=1\n",
    "    if gen_iterations%display_iters==0:\n",
    "        if gen_iterations%(5*display_iters)==0:\n",
    "            clear_output()\n",
    "        print('[%d/%d][%d] Loss_D: %f Loss_G: %f loss_L1: %f'\n",
    "        % (epoch, niter, gen_iterations, errD_sum/display_iters, errG_sum/display_iters, errL1_sum/display_iters), time.time()-t0)\n",
    "        _, valA, valB = train_batch.send(6) \n",
    "        fakeB = netG_gen(valA)\n",
    "        showX(np.concatenate([valA, valB, fakeB], axis=0), 3)\n",
    "        errL1_sum = errG_sum = errD_sum = 0\n",
    "        _, valA, valB = next(val_batch)\n",
    "        fakeB = netG_gen(valA)\n",
    "        showX(np.concatenate([valA, valB, fakeB], axis=0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, valA, valB = train_batch.send(6) \n",
    "fakeB = netG_gen(valA)\n",
    "showX(np.concatenate([valA, valB, fakeB], axis=0), 3)\n",
    "errL1_sum = errG_sum = errD_sum = 0\n",
    "_, valA, valB = next(val_batch)\n",
    "fakeB = netG_gen(valA)\n",
    "showX(np.concatenate([valA, valB, fakeB], axis=0), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
